{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XlfQas3_AJwf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c)\n",
        "# SANTE 2023\n",
        "# Philippe Bouchet 2022\n",
        "# philippe.bouchet@epita.fr"
      ],
      "metadata": {
        "id": "5SuyW_ql7raj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Workshop - Google Developer Student Club EPITA\n",
        "\n",
        "Welcome to GDSC EPITA's first workshop!\n",
        "\n",
        "In this notebook we will be discovering the very basics of Deep Learning with Python's TensorFlow library!\n",
        "This notebook is meant to be accessible for people who have never used TensorFlow (or NumPy) before! An adept understanding of Python is required to finish this notebook.\n",
        "\n",
        "Remember:\n",
        "- You can discuss with your neighbours, and exchange ideas so that you can improve your models!\n",
        "- If you don't have enough time to finish the notebook here, you can always take your time at home!\n",
        "\n",
        "- If you have any issues with the notebook, don't hesitate to ask for help, or send a message on the Discord!\n",
        "\n",
        "Good luck!"
      ],
      "metadata": {
        "id": "-MdPHeHGZVnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment\n",
        "\n",
        "In order to accomplish this task, we will need to load some Python libraries.\n",
        "We are going to use:\n",
        "- **Tensorflow 2.0**: For building and training our deep learning model\n",
        "- **Numpy**: For linear algebra, and ndarray operations\n",
        "- **Matplotlib**: To visualize the data"
      ],
      "metadata": {
        "id": "3VF5UqMCHfK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with importing tensorflow into our work environment\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Fixed seed for deterministic results\n",
        "tf.random.set_seed(0)"
      ],
      "metadata": {
        "id": "k9KMF667cqIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy for linear algebra\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_kAx_zOqHLX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matplotlib and Seaborn for data visualization\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2hKoNPc-GWGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be on Google colab, we will need to make use of the GPUs.\n",
        "To enable GPUs on your session, go on `Edit`, then `Notebook settings` and select `GPU` as the hardware accelerator."
      ],
      "metadata": {
        "id": "yY2cB8gj8Cbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A brief introduction to NumPy\n",
        "\n",
        "This section is meant for those who have never used NumPy. If you're already familiar with NumPy, **then you can skip this section**.\n",
        "\n",
        "NumPy, or *Numerical Python* is a library dedicated to make numerical computing with Python very easy and straightforward. We will not go too in depth with NumPy, so all you need to know for now is how to use the `np.ndarray` type.\n",
        "\n",
        "What is an `np.ndarray` type? It stands for \"*N-Dimensional Array*\". This type is very powerful, as it allows you to manipulate multidimensional arrays quickly and with ease, as opposed to standard lists in Python.\n",
        "\n",
        "Here are a few ways that we can use the `np.ndarray`type:"
      ],
      "metadata": {
        "id": "XlfQas3_AJwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing an array"
      ],
      "metadata": {
        "id": "35qQMbtMDHzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_array = np.array([1, 2, 3, 4])  # Initialize from list\n",
        "array = np.ones((2, 2)) # Initialize a 2x2 (2D) array full of ones\n",
        "\n",
        "print(f\"arr:\\n{simple_array}\\n\")\n",
        "print(f\"arr_ones:\\n{array}\")"
      ],
      "metadata": {
        "id": "LJ1XUBtBBMfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the array's dimensions using its `shape` attribute"
      ],
      "metadata": {
        "id": "OKVnNnExBuW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array.shape  # Shape of (2x2) "
      ],
      "metadata": {
        "id": "tpnTiH6aB44T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing array wide operations quickly and efficiently\n"
      ],
      "metadata": {
        "id": "liCPSounCKyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array += 1 # Add 1 to every element in array\n",
        "print(f\"Increment array:\\n{array}\\n\")\n",
        "\n",
        "array = np.sin(array) # Apply the sine function to every element in array\n",
        "print(f\"Sine function applied to array:\\n{array}\")"
      ],
      "metadata": {
        "id": "L_esDYCICJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "The task in this workshop is that of image **classification**.\n",
        "\n",
        "We seek to determine if the contents of an image corresponds to one of the classes from the dataset that was used to train our network. A class simply represents the label that is associated to data from the dataset.\n",
        "\n",
        "For example, imagine we have a dataset that contains 50000 images, of horses, shoes and guitars. We have only 3 **classes** here. We can use this dataset to train our network to classify images of horses, shoes and guitars.\n",
        "\n",
        "But can you guess what happens when you try to give in an image of an airplane to your network? It will probably think it's a horse! This network is specialized **only** for classifying those 3 classes."
      ],
      "metadata": {
        "id": "Q7U8Zjb9HCQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "Let's start off by loading our dataset! It's really important to correctly load our dataset. In most real life situations, we will be confronted with corrupted datasets, often populated with invalid pieces of data. However, the dataset that we will be using for this workshop (**cifar10**) is a perfectly clean, and rich dataset! So we can jump straight into writing the network once we load and verify our data! \n",
        "\n",
        "### What are $X$ and $y$?\n",
        "\n",
        "In **supervised learning**, we call $X$ the input data, and $y$ the label data. When training our model, we will input $X$, and the model will predict a variable $\\hat{y}$. In order to measure the difference between the predicted value ($\\hat{y}$), and the expected value ($y$), we use a *loss function*\n",
        "We call this loss function $L$:\n",
        "\n",
        "$$\n",
        "L_{CE}(\\hat{y},y) = - \\sum_{i=0}^{N}y_i log(\\hat{y_i})\n",
        "$$\n",
        "\n",
        "This is the formula for **cross entropy** ($CE$).\n",
        "\n",
        "We will be using **categorical cross entropy** ($CCE$) for this workshop.\n",
        "\n",
        "$$\n",
        "L_{CCE}(\\hat{y}, y) = -\\frac{1}{N}\\sum_{i=0}^{N}{\\sum_{j=0}^{J}{y_j log(\\hat{y_j})+(1-y_j) log(1 - \\hat{y_j})}}\n",
        "$$\n",
        "\n",
        "No need to memorize these formulas! It's just to give you an idea of how the network calculates the error between the predictions and the expected values.\n",
        "\n",
        "The network then uses **gradient descent** to optimize the parameters, with respect to the variations in the loss function, by using the chain rule.\n",
        "\n",
        "We'll try and understand the mathematical aspect of deep learning in another workshop perhaps... ;) \n",
        "\n",
        "### Train and test data, why split the dataset?\n",
        "\n",
        "In Deep Learning, and by extent Machine Learning, it's absolutely vital to separate our data into training, and testing datasets.\n",
        "\n",
        "Why, you ask? When you train the network with all of the data from the dataset, the model develops bias towards this data. So if we were to use that same data to evaluate the model's accuracy, our result would be biased.\n",
        "We have to use data that the model has never seen before!\n",
        "\n",
        "One way to do this is by splitting dataset into two:\n",
        "- **Training dataset**: This dataset will be used to train the model.\n",
        "- **Test dataset**: This data will be used to evaluate the model's accuracy once it has been trained.\n",
        "\n",
        "We'll start off by loading the MNIST handwritten dataset. It contains images of handwritten digits from 0, to 9. This dataset has **10** classes!"
      ],
      "metadata": {
        "id": "wrPFYnOILwoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Change shape from (60000, 1) -> (60000,)\n",
        "y_train = y_train.flatten()\n",
        "y_test  = y_test.flatten()"
      ],
      "metadata": {
        "id": "4TComceHEsuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the data\n",
        "\n",
        "Let's take a look at the data we just loaded!"
      ],
      "metadata": {
        "id": "vws-FlK58dfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "5rC4iuo3FcKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "hyKTu99LsM_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the builtin `shape` attribute from `X_train`, we get a tuple containing 3 elements. 60000, 28, 28. These elements correspond to the dimensions of our data.\n",
        "\n",
        "We have 60000 elements, with each element having a dimension of 28x28. These dimensions correspond to image dimensions. An image of width 28 and of height 28.\n",
        "\n",
        "If we had an RGB image, we would have an extra dimension. We'd simply have a tuple that would look like `(60000, 28, 28, 3)`.\n",
        "\n",
        "Let's go ahead and visualize our data!"
      ],
      "metadata": {
        "id": "M5w-11ESbZcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idx - Select a random index between 0 and 50000\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "\n",
        "img_data  = X_train[idx] # Data containing the image\n",
        "\n",
        "_ = plt.imshow(X_train[idx])"
      ],
      "metadata": {
        "id": "hcaQpR8TGUEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see an image from our dataset! Amazing! \n",
        "\n",
        "Now, let's try and see the corresponding label for this image!"
      ],
      "metadata": {
        "id": "Ku4gDE-hdzWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"All possible y values: {np.unique(y_train)}\")\n",
        "print(f\"\\nLabel for image {idx}: {y_train[idx]}\")"
      ],
      "metadata": {
        "id": "RTdLpeRkLczP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing\n",
        "\n",
        "Before we can start feeding our network with the data, we're gonna have to process it a bit. This step is called **pre-processing** since it will be carried out *before* it is passed on to the network.\n",
        "\n",
        "We're gonna look at **one-hot encoding** for our y labels, and **normalization** for our X values."
      ],
      "metadata": {
        "id": "cj49Qw6PRTF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding\n",
        "\n",
        "Since we have 10 different classes, our labels vary from 0 to 9.\n",
        "\n",
        "However, our network can't have a single output that predicts from 0 to 9! Since we are bound by the properties of the softmax activation function, we can only have an output that ranges continuously from 0 to 1!\n",
        "\n",
        "To solve this, we need a network with 10 different outputs, with each output corresponding to a certain class. Now our network is be able to predict these different classes! So we can have 10 output neurons in this case, and a neuron whose value is close to `1` means that it predicted an element from that class!\n",
        "\n",
        "But our label values ranges from 0 to 9, so how can we make our labels compatible with this network of 10 different outputs?\n",
        "\n",
        "We will be using \"*one-hot encoding*\"!\n",
        "\n",
        "The concept is simple, given a class of $n$ possible values, we will create vectors of size $n$ for each label. \n",
        "\n",
        "With $n = 10$ for this situation.\n",
        "\n",
        "For each vector, the element at the corresponding value of the label will be set to 1, whereas all the others will be set to 0.\n",
        "\n",
        "For example:\n",
        "\n",
        "The label for the number `3` the vector `[0,0,0,1,0,0,0,0,0,0]`\n",
        "\n",
        "The label for the number `9` becomes the vector `[0,0,0,0,0,0,0,0,0,1]`\n",
        "\n",
        "So on and so forth...\n",
        "\n",
        "Here's how to use One hot encoding in TensorFlow! "
      ],
      "metadata": {
        "id": "dkJvFwFZLrlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "# depth - corresponds to the number of classes we wish to encode\n",
        "y_train = tf.one_hot(y_train, depth=10)"
      ],
      "metadata": {
        "id": "zmD86hf9NpsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see what we get!"
      ],
      "metadata": {
        "id": "E9-LWTuBQZEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLabel for image {idx}: {y_train[idx]}\")"
      ],
      "metadata": {
        "id": "0Z_1pIEiQY6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our labels are now encoded in a way that is understandable by our network!"
      ],
      "metadata": {
        "id": "fymzuuNhRGPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "Values in 8 bit images range from 0 to 255. However, this strong variance in pixel values can result in the network struggling to find the local minimum during gradient descent. Finding the local minimum essentially allows the network to update itself with the best parameters.\n",
        "\n",
        "Since we want our network to have the best possible parameters, we're going to have to help it find the local minimum by **normalizing** our input data.\n",
        "\n",
        "Let's take a look at a pixel value's range of an image in our dataset."
      ],
      "metadata": {
        "id": "kyGhqcMQRrwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Range for X: [{X_train[idx].min()}, {X_train[idx].max()}]\")"
      ],
      "metadata": {
        "id": "0wxAiBVIlV1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can notice that our pixel values vary too much!\n",
        "\n",
        "Pixel values are usually defined within $[0, 255]$. If we can *normalize* the range from $[0, 255]$ to $[0, 1]$, then our network can find find the best parameters to update itself with.\n",
        "\n",
        "Let's code the normalization function!\n",
        "\n",
        "https://en.wikipedia.org/wiki/Normalization_(image_processing)\n",
        "\n",
        "If you're interested you can also read [this article](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_9_Normalized.html) about normalization."
      ],
      "metadata": {
        "id": "ec_2agff5052"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### FIXME - Normalization function"
      ],
      "metadata": {
        "id": "FBlE2DZq8UJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments:\n",
        "#   X - np.ndarray - Values ranging from [0, 255]\n",
        "#\n",
        "# Return:\n",
        "#   X_norm - Normalized np.ndarray in the range [0, 1]\n",
        "#\n",
        "# Hint: This function can be coded in one line! ;)\n",
        "def normalize(X : np.ndarray) -> np.ndarray:\n",
        "  return X/255"
      ],
      "metadata": {
        "id": "9YYflXMeGnd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "CCuQDZrjfnKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = normalize(X_train)\n",
        "print(f\"Range for X normalized: [{X_train[idx].min()} , {X_train[idx].max()}]\")\n"
      ],
      "metadata": {
        "id": "l7_IfpK_fo_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you imagine what the normalized image looks like? (*Don't peek at the answer below!*)"
      ],
      "metadata": {
        "id": "W1xwZkAogTtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.imshow(X_train[idx])"
      ],
      "metadata": {
        "id": "q_7X5FcKf05e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's right, it looks exactly the same! Normalization is a linear operation, therefor the pixel's value is proportionnaly scaled from $[0,255]$ to $[0,1]$."
      ],
      "metadata": {
        "id": "XKRmgXefggv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "\n",
        "Now that we know how to load our data, and normalize it, let's design the model!\n",
        "\n",
        "For this task, we have opted for a **Convolutional Neural Network** (CNN) model, since we want to be able to classify images.\n",
        "\n",
        "We will be using the Keras API for the structure of our network.\n",
        "\n",
        "It does seem overwhelming at first, but this is definitely within your reach! It will take a bit of time to understand, so if you are having trouble with designing the network later on in the notebook, you can inspire yourself with this model's structure!\n",
        "\n",
        "You can always refer to the slides from the presentation on CNN models."
      ],
      "metadata": {
        "id": "Z4jUlJI78M5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of the model"
      ],
      "metadata": {
        "id": "VnhYrhEpzNO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start off by defining the general structure of our model!\n",
        "\n",
        "We call the `Sequential()` method from the Keras `models` API.\n",
        "\n",
        "For the initializer to this method, we can pass a list containing different layers. The way this list is constructed allows us to build models in a very efficient, lego-like way, as if we were using building blocks.\n",
        "\n",
        "It's important to note that the first layer *must* always have a keyword argument specifying the input shape!"
      ],
      "metadata": {
        "id": "xyngEXwLTXfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the structure of the model.\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, 7, padding='same', activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10,  activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "9rjH2pVUFLKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break this down a little!\n",
        "\n",
        "- `layers.Conv2D()` creates a layer in our network dedicated to applying the convolution on the image. The first two integer parameters correspond to the amount of kernels we will be using, and the size of the kernel, respectively.\n",
        "- `layers.MaxPooling2D()` creates another layer in the network dedicated to resizing the image using the max-pool algorithm.\n",
        "- `layers.Flatten()` allows us to transition from a multi-dimensional array shape to a 1-dimensional vector.\n",
        "- `layers.Dense()` creates a fully connected layer. The first integer parameter corresponds to the number of neurons in the layer. By default, the final dense layer will be counted as the output layer.\n",
        "\n",
        "We also notice that most layers take an `activation` parameter, with the final layer being different.\n",
        "\n",
        "- `relu` is an activation that is used during training of the network. It is especially useful in visual feature extraction, as it sets all negative values to 0, and keeps all positive values. We use it for all intermediary layers.\n",
        "\n",
        "- `softmax` is an activation that we use at the very end of the network. It will activate the output neurons, and assign a value between $[0,1]$. We will use it for the final layer."
      ],
      "metadata": {
        "id": "y2lGev2oTc4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model parameters"
      ],
      "metadata": {
        "id": "habANSJFzPZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've constructed our network, we'll have to specify the Loss function, the optimizer and the metrics of the network!\n",
        "\n",
        "We can do so by calling the model's `compile` method:\n"
      ],
      "metadata": {
        "id": "UD3As9ZElygW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"SGD\",\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "T0LNG_4uTdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you may be asking yourself what exactly the values to these variables are.\n",
        "\n",
        "- `optimizer` corresponds to the algorithm that is used during gradient descent! Some are better than others, you can always search for a list of optimizers in TensorFlow! In here we have `SGD` which is short for `Stochastic Gradient Descent`, it is the standard algorithm used for gradient descent.\n",
        "\n",
        "- `loss` corresponds to the function that will calculate the rate of error between our predicted value, and our expected value. `categorical_crossentropy` is used in this example, as it is useful for detecting different classes.\n",
        "\n",
        "- `metrics` takes a list of different metrics to be evaluated during model training. By passing a list containing only `accuracy` we tell the model that we want to see how its accuracy evolves at each epoch."
      ],
      "metadata": {
        "id": "B0G8FD9orseD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `model.summary()` once you compiled your model, to take a look at how it's configured!"
      ],
      "metadata": {
        "id": "lwXCwWtsOazW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bMcH6Sn3OfMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "FWV0bmLJzT8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've compiled our model, we can call the `fit` method from our model with our training dataset, `X_train` and `y_train`.\n",
        "\n",
        "We can see that we're calling this method with two other positional keywords. `epochs` and `batch_size`\n",
        "\n",
        "- `epochs` corresponds to the amount of rounds in which we will update the network's parameters with gradient descent.\n",
        "- `batch_size` during a single epoch, the network will randomly select `batch_size` amount of data from our `X_train` and `y_train` dataset, and use them to train the network for the entirety of that epoch.\n",
        "\n",
        "We also see that we have `validation_data` set to the test dataset. The reason why we have this is because the `accuracy` metric is not representative of the model's ability to perform on new data. It shows the accuracy on data it was already trained with.\n",
        "\n",
        "In general, we want `val_accuracy` and `accuracy` to be always on the same level. The moment our `val_accuracy` is significantly lower than the `accuracy` then our model is not generalizing well on new data. This is called ***overfitting***.\n",
        "\n",
        "In order to have an idea of how well the network performs on new data during the training, we can use the test dataset as validation data.\n",
        "\n",
        "Since the validation data does not interfere when tuning the model's paramters, we can also use it later on to evaluate the final model's performance."
      ],
      "metadata": {
        "id": "2ZToB6IlrWc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must preprocess the test dataset the same way as the train dataset before using it for validation."
      ],
      "metadata": {
        "id": "45BSMfUajuzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize X_test\n",
        "X_test = normalize(X_test)\n",
        "\n",
        "# One-hot encoding for y_test\n",
        "y_test  = tf.one_hot(y_test, depth=10)"
      ],
      "metadata": {
        "id": "sqKOCxvej1SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
      ],
      "metadata": {
        "id": "p978extWF4zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model"
      ],
      "metadata": {
        "id": "IsaDMDMrzWkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've finished training our network, we can evaluate its performance by using the test dataset!"
      ],
      "metadata": {
        "id": "82sML89nuXcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate our model with the `evaluate` method! We call this method with the test dataset.\n",
        "\n",
        "This method will return a tuple containing the loss, and the accuracy."
      ],
      "metadata": {
        "id": "B4UveV8hxYHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "Uoj_aWTgxXdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"With the test dataset, the model has:\\n\\n\"\n",
        "      f\"An accuracy of:\\t{model_result[1]*100:.2f}% on new data\\n\"\n",
        "      f\"A loss of:\\t{model_result[0]:.2f}\")"
      ],
      "metadata": {
        "id": "QNjSW6XvJHEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we interpret these results?\n",
        "\n",
        "Well for the accuracy it's simple, the closer it is to 100%, the more accuracte our network is on a dataset. This indicates how well our model performs on data that it has never seen before, it is a good indicator of how generalized our model is.\n",
        "\n",
        "The value of the loss indicates the rate of error the network made during the prediction on the dataset.\n",
        "\n",
        "The higher it is, then the model can not generalize very well on new data.\n",
        "\n",
        "The lower it is, the better."
      ],
      "metadata": {
        "id": "dUz5hEsEx-Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and predict the value of an image from the test dataset!"
      ],
      "metadata": {
        "id": "dHkXArFI3_vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted labels for every image in test dataset\n",
        "# Only need to run once!\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "yON3r1DY4jc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idx - Select a random image from test dataset\n",
        "idx = np.random.randint(X_test.shape[0])\n",
        "\n",
        "# Select the index from the one hot encoded vector\n",
        "# that corresponds to the number. Since we're\n",
        "# dealing with MNIST, the index corresponds to \n",
        "# the number we're trying to predict.\n",
        "# This is NOT the case for other datasets, such as\n",
        "# cifar10!\n",
        "y_test_val = tf.argmax(y_test[idx])\n",
        "y_pred_val = tf.argmax(y_pred[idx])\n",
        "\n",
        "print(f\"Expected value for image {idx}:\\t{y_test_val}\\n\"\n",
        "      f\"Predicted value for image {idx}:\\t{y_pred_val}\")\n",
        "\n",
        "# Display randomly selected image\n",
        "_ = plt.imshow(X_test[idx])"
      ],
      "metadata": {
        "id": "548JsSB42SLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn!"
      ],
      "metadata": {
        "id": "iVGwDW-D1Jv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did it! We trained a CNN model to classify images from the MNIST handwritten digits dataset! Now it's your turn!\n",
        "The dataset that you'll be working on is the cifar10 digits dataset.\n",
        "\n",
        "This dataset contains images of airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships and trucks.\n",
        "\n",
        "We loaded the X and y train and test datasets, the rest is up to you!\n",
        "\n",
        "If you have any questions, feel free to ask!\n",
        "\n",
        "Good luck!\n"
      ],
      "metadata": {
        "id": "-4dCG9fAntMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus challenge"
      ],
      "metadata": {
        "id": "DzCDO1RI1WvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you ***really*** want to challenge yourself, try and make a network that scores more than **75%** on the test dataset!\n",
        "\n",
        "Experiment with different layers, parameters, activation functions, and optimizers! Try training your model with a different batch size, or more epochs!\n",
        "\n",
        "See what works best! You can even use data augmentation to enrich your dataset, adding more variability to your network!\n"
      ],
      "metadata": {
        "id": "C4FtbbIy1C5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoded value for each class for reference:\n",
        "\n",
        "```\n",
        "0. airplane\n",
        "1. automobile\n",
        "2. bird\n",
        "3. cat\n",
        "4. deer\n",
        "5. dog\n",
        "6. frog\n",
        "7. horse\n",
        "8. ship\n",
        "9. truck\n",
        "```"
      ],
      "metadata": {
        "id": "MpexdfHDp_z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Change shape from (50000, 1) -> (50000,)\n",
        "y_train = y_train.flatten()\n",
        "y_test  = y_test.flatten()"
      ],
      "metadata": {
        "id": "6KcTcLcKo36s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class names for easier understanding of the label\n",
        "# Each index corresponds to the class\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "0yXLhA56qHNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Check the dimensions of your data\n",
        "print(f\"X_train dimensions: {X_train.shape}\")\n",
        "print(f\"y_train dimensions: {y_train.shape}\")"
      ],
      "metadata": {
        "id": "Bj7PHEXBpnSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Use matplotlib to visualize the images\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "label = class_names[y_train[idx]]\n",
        "\n",
        "print(f\"Image is: {label}\")\n",
        "\n",
        "_ = plt.imshow(X_train[idx])"
      ],
      "metadata": {
        "id": "aMxBmOQ2K-oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Preprocess the train data\n",
        "X_train = normalize(X_train)\n",
        "y_train = tf.one_hot(y_train, depth=10)"
      ],
      "metadata": {
        "id": "SE--Gg5JpLH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Preprocess the test dataset\n",
        "X_test = normalize(X_test)\n",
        "y_test = tf.one_hot(y_test, depth=10)"
      ],
      "metadata": {
        "id": "HlN-SYh-PIad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Create your model's structure with models.Sequential()\n",
        "model = models.Sequential([  \n",
        "    layers.Conv2D(64, 5, padding='same', activation='relu'),\n",
        "    layers.Conv2D(64, 5, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
        "    layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "  \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128,  activation='relu'),\n",
        "    layers.Dense(10,  activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "2hM3lEya-BdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Compile your model, specify the optimizer, loss and metrics\n",
        "model.compile(optimizer=\"Adam\",\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vSttc7jupZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Run the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=128)"
      ],
      "metadata": {
        "id": "I7tyV62x0eQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXME - Evaluate your model on the test dataset\n",
        "model_result = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"\\nWith the test dataset, the model has:\\n\\n\"\n",
        "      f\"An accuracy of:\\t{model_result[1]*100:.2f}% on new data\\n\"\n",
        "      f\"A loss of:\\t{model_result[0]:.2f}\")"
      ],
      "metadata": {
        "id": "D_nsZbGRo0cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}